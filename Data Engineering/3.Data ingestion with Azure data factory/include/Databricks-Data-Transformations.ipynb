{"cells":[{"cell_type":"markdown","source":["# 03 Data Ingestion via ADF - Data Transformations Notebook"],"metadata":{}},{"cell_type":"markdown","source":["This notebook performs the data munging and transformation tasks against the files ingested via ADF in the Data Ingestion notebook for this module. This notebook will be executed by a call from Azure Data Factory (ADF), and will automate the process of data munging and table creation for the data used within this module.\n\nThe following table will be created by this notebook:\n\n- homicides_2016"],"metadata":{}},{"cell_type":"code","source":["# Create input widgets, which will accept parameters passed in via the ADF Databricks Notebook activity\ndbutils.widgets.text(\"accountName\", \"\", \"Account Name\")\ndbutils.widgets.text(\"accountKey\", \"\", \"Account Key\")\ndbutils.widgets.text(\"containerName\", \"\", \"Container Name\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Assign variables to the passed in values of the widgets\naccountName = dbutils.widgets.get(\"accountName\")\naccountKey = dbutils.widgets.get(\"accountKey\")\ncontainerName = dbutils.widgets.get(\"containerName\")\n\n# Create connection string to use for accessing files in the storage account\nconnectionString = \"wasbs://%(containerName)s@%(accountName)s.blob.core.windows.net/03.02\" % locals()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Create connection to the Azure Storage account\nspark.conf.set(\"fs.azure.account.key.\" + accountName + \".blob.core.windows.net\", accountKey)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### Create DataFrame for each city's crime data"],"metadata":{}},{"cell_type":"code","source":["bostonDf = spark.read.parquet(\"%(connectionString)s/Crime-Data-Boston-2016.parquet\" % locals())\nchicagoDf = spark.read.parquet(\"%(connectionString)s/Crime-Data-Chicago-2016.parquet\" % locals())\ndallasDf = spark.read.parquet(\"%(connectionString)s/Crime-Data-Dallas-2016.parquet\" % locals())\nlosAngelesDf = spark.read.parquet(\"%(connectionString)s/Crime-Data-Los-Angeles-2016.parquet\" % locals())\nnewOrleansDf = spark.read.parquet(\"%(connectionString)s/Crime-Data-New-Orleans-2016.parquet\" % locals())\nnewYorkDf = spark.read.parquet(\"%(connectionString)s/Crime-Data-New-York-2016.parquet\" % locals())\nphillyDf = spark.read.parquet(\"%(connectionString)s/Crime-Data-Philadelphia-2016.parquet\" % locals())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1395183622383482&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>bostonDf <span class=\"ansiyellow\">=</span> spark<span class=\"ansiyellow\">.</span>read<span class=\"ansiyellow\">.</span>parquet<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;%(connectionString)s/Crime-Data-Boston-2016.parquet&quot;</span> <span class=\"ansiyellow\">%</span> locals<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> chicagoDf <span class=\"ansiyellow\">=</span> spark<span class=\"ansiyellow\">.</span>read<span class=\"ansiyellow\">.</span>parquet<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;%(connectionString)s/Crime-Data-Chicago-2016.parquet&quot;</span> <span class=\"ansiyellow\">%</span> locals<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> dallasDf <span class=\"ansiyellow\">=</span> spark<span class=\"ansiyellow\">.</span>read<span class=\"ansiyellow\">.</span>parquet<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;%(connectionString)s/Crime-Data-Dallas-2016.parquet&quot;</span> <span class=\"ansiyellow\">%</span> locals<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> losAngelesDf <span class=\"ansiyellow\">=</span> spark<span class=\"ansiyellow\">.</span>read<span class=\"ansiyellow\">.</span>parquet<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;%(connectionString)s/Crime-Data-Los-Angeles-2016.parquet&quot;</span> <span class=\"ansiyellow\">%</span> locals<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span> newOrleansDf <span class=\"ansiyellow\">=</span> spark<span class=\"ansiyellow\">.</span>read<span class=\"ansiyellow\">.</span>parquet<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;%(connectionString)s/Crime-Data-New-Orleans-2016.parquet&quot;</span> <span class=\"ansiyellow\">%</span> locals<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansicyan\">parquet</span><span class=\"ansiblue\">(self, *paths)</span>\n<span class=\"ansigreen\">    314</span>         <span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;name&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;string&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;year&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;int&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;month&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;int&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;day&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;int&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    315</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 316</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_df<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jreader<span class=\"ansiyellow\">.</span>parquet<span class=\"ansiyellow\">(</span>_to_seq<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_spark<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">,</span> paths<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    317</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    318</span>     <span class=\"ansiyellow\">@</span>ignore_unicode_prefix<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    327</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 328</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    329</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    330</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o297.parquet.\n: shaded.databricks.org.apache.hadoop.fs.azure.AzureException: java.lang.IllegalArgumentException: URI &apos;wasbs://@.blob.core.windows.net/03.02/Crime-Data-Boston-2016.parquet&apos; has a malformed WASB authority, expected container name.Authority takes the form wasb://[&lt;container name&gt;@]&lt;account name&gt;\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.createAzureStorageSession(AzureNativeFileSystemStore.java:1031)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.initialize(AzureNativeFileSystemStore.java:482)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.initialize(NativeAzureFileSystem.java:1279)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:96)\n\tat org.apache.spark.sql.DataFrameReader.preprocessDeltaLoading(DataFrameReader.scala:226)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:258)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:724)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: URI &apos;wasbs://@.blob.core.windows.net/03.02/Crime-Data-Boston-2016.parquet&apos; has a malformed WASB authority, expected container name.Authority takes the form wasb://[&lt;container name&gt;@]&lt;account name&gt;\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.getContainerFromAuthority(AzureNativeFileSystemStore.java:614)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.createAzureStorageSession(AzureNativeFileSystemStore.java:963)\n\t... 20 more\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["### Create normalized DataFrames for each city\n\nFor the upcoming aggregation, you need to alter the data sets to include a `month` column which can be computed from the various \"Incident Date\" columns using the `month()` function. Boston already has this column.\n\nIn this example, we use several functions in the `pyspark.sql.functions` library, and need to import:\n\n* `col()` to return a column from a DataFrame, based on the given column name.\n* `contains(mySubstr)` to indicate a string contains substring `mySubstr`.\n* `lit()` to create a column from a literal value.\n* `lower()` to convert text to lowercase.\n* `month()` to extract the month from `reportDate` timestamp data type.\n* `unix_timestamp()` to convert the Dallas date field into a timestamp format, so the month can be extracted using the `month()` function.\n\nAlso, note:\n\n* We use  `|`  to indicate a logical `or` of two conditions in the `filter` method.\n* We use `lit()` to create a new column in each DataFrame containing the name of the city for which the data is derived."],"metadata":{}},{"cell_type":"code","source":["# Import required libraries\nimport datetime\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import col, lit, lower, month, unix_timestamp, upper"],"metadata":{"collapsed":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":["homicidesBostonDf = (bostonDf.withColumn(\"city\", lit(\"Boston\"))\n  .select(\"month\", col(\"OFFENSE_CODE_GROUP\").alias(\"offense\"), col(\"city\"))\n  .filter(lower(col(\"OFFENSE_CODE_GROUP\")).contains(\"homicide\"))\n)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["homicidesChicagoDf = (chicagoDf.withColumn(\"city\", lit(\"Chicago\"))\n  .select(month(col(\"date\")).alias(\"month\"), col(\"primaryType\").alias(\"offense\"), col(\"city\"))\n  .filter(lower(col(\"primaryType\")).contains(\"homicide\"))\n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["homicidesDallasDf = (dallasDf.withColumn(\"city\", lit(\"Dallas\"))\n   .select(month(unix_timestamp(col(\"callDateTime\"),\"M/d/yyyy h:mm:ss a\").cast(\"timestamp\")).alias(\"month\"), col(\"typeOfIncident\").alias(\"offense\"), col(\"city\"))\n   .filter(lower(col(\"typeOfIncident\")).contains(\"murder\") | lower(col(\"typeOfIncident\")).contains(\"manslaughter\"))\n)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["homicidesLosAngelesDf = (losAngelesDf.withColumn(\"city\", lit(\"Los Angeles\"))\n   .select(month(col(\"dateOccurred\")).alias(\"month\"), col(\"crimeCodeDescription\").alias(\"offense\"), col(\"city\"))\n   .filter(lower(col(\"crimeCodeDescription\")).contains(\"homicide\") | lower(col(\"crimeCodeDescription\")).contains(\"manslaughter\"))\n)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["homicidesNewOrleansDf = (newOrleansDf.withColumn(\"city\", lit(\"New Orleans\"))\n   .select(month(col(\"Occurred_Date_Time\")).alias(\"month\"), col(\"Incident_Description\").alias(\"offense\"), col(\"city\"))\n   .filter(lower(col(\"Incident_Description\")).contains(\"homicide\") | lower(col(\"Incident_Description\")).contains(\"murder\"))\n)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["homicidesNewYorkDf = (newYorkDf.withColumn(\"city\", lit(\"New York\"))\n  .select(month(col(\"reportDate\")).alias(\"month\"), col(\"offenseDescription\").alias(\"offense\"), col(\"city\")) \n  .filter(lower(col(\"offenseDescription\")).contains(\"murder\") | lower(col(\"offenseDescription\")).contains(\"homicide\"))\n)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["homicidesPhillyDf = (phillyDf.withColumn(\"city\", lit(\"Philadelphia\"))\n   .select(month(col(\"dispatch_date\")).alias(\"month\"), col(\"text_general_code\").alias(\"offense\"), col(\"city\"))\n   .filter(lower(col(\"text_general_code\")).contains(\"homicide\"))\n)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### Create a single DataFrame\nWith the normalized homicide data for each city, combine the two by taking their union."],"metadata":{}},{"cell_type":"code","source":["homicidesDf = homicidesNewYorkDf.union(homicidesBostonDf).union(homicidesChicagoDf).union(homicidesDallasDf).union(homicidesLosAngelesDf).union(homicidesNewOrleansDf).union(homicidesPhillyDf)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Export the prepared data to a persistent table"],"metadata":{}},{"cell_type":"code","source":["homicidesDf.write.mode(\"overwrite\").saveAsTable(\"homicides_2016\")"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Return OK status"],"metadata":{}},{"cell_type":"code","source":["import json\ndbutils.notebook.exit(json.dumps({\n  \"status\": \"OK\",\n  \"message\": \"Cleaned data and created persistent table\",\n  \"tables\": [\"homicides_2016\"]\n}))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/plain":["{\"tables\": [\"homicides_2016\"], \"status\": \"OK\", \"message\": \"Cleaned data and created persistent table\"}"]}}],"execution_count":22},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"language_info":{"codemirror_mode":{"name":"python","version":"3"},"mimetype":"text/x-python","name":"pyspark3","pygments_lexer":"python3"},"name":"Databricks-Data-Transformations","notebookId":1395183622383475,"kernelspec":{"display_name":"PySpark3","language":"","name":"pyspark3kernel"},"celltoolbar":"Raw Cell Format"},"nbformat":4,"nbformat_minor":0}
