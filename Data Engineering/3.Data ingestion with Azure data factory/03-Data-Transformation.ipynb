{"cells":[{"cell_type":"markdown","source":["# Data Transformation via Azure Data Factory\n\nAs you saw at the end of the previous lesson, different cities use different field names and values to indicate crimes, dates, etc. within their crime data.\n\nFor example:\n* Some cities use the value \"HOMICIDE\", \"CRIMINAL HOMICIDE\" or \"MURDER\".\n* In the New York data, the column is named `offenseDescription` while in the Boston data, the column is named `OFFENSE_CODE_GROUP`.\n* In the New York data, the date of the event is in the `reportDate`, while in the Boston data, there is a single column named `MONTH`.\n\nIn the case of New York and Boston, here are the unique characteristics of each data set:\n\n| | Offense-Column        | Offense-Value          | Reported-Column  | Reported-Data Type |\n|-|-----------------------|------------------------|-----------------------------------|\n| New York | `offenseDescription`  | starts with \"murder\" or \"homicide\" | `reportDate`     | `timestamp`    |\n| Boston | `OFFENSE_CODE_GROUP`  | \"Homicide\"             | `MONTH`          | `integer`      |\n\nIn this notebook, we will use an ADF Databricks Notebooks activity to perform transformations on and extract homicide statistics from the crime data being ingested via ADF.\n\nIn this lesson you:\n1. Create Databricks Access Token.\n2. Add Databricks Notebook activity to pipeline.\n3. Connect Copy Activities to Notebook Activity.\n4. Publish the updated pipeline.\n5. Trigger and Monitor the pipeline run.\n6. Verify transformations of data by looking at the generated table in Databricks.\n7. Perform a simple aggregation of the data."],"metadata":{}},{"cell_type":"markdown","source":["## Step 1: Create Databricks Access Token\n\nIn this step, you will generate an access token that will be used by ADF to access your Databricks workspace.\n\nIn the top right corner of your Databricks workspace (this window), select the Account icon, and then select **User Settings**.\n  \n  ![Databricks account menu](https://databricksdemostore.blob.core.windows.net/images/03/03/databricks-account-menu.png \"Databricks account menu\")\n  \nOn the User Settings screen, select Generate New Token.\n  \n  ![User Settings](https://databricksdemostore.blob.core.windows.net/images/03/03/user-settings.png)\n\nIn the Generate New Token dialog, enter a comment, such as \"ADF access\", leave the Lifetime set to 90 days, and select **Generate**.\n  \n  ![Generate New Token](https://databricksdemostore.blob.core.windows.net/images/03/03/generate-new-token.png)\n  \nCopy the generated token and paste it into a text editor, such as Notepad.exe, for use when setting up the Databricks Linked Service in ADF.\n\n> Note: Do not close the dialog containing the generated token until have have save it in a text editor, as it will not be visible again once you close the dialog.\n  \n  ![Copy Token](https://databricksdemostore.blob.core.windows.net/images/03/03/copy-token.png)"],"metadata":{}},{"cell_type":"markdown","source":["## Step 2: Add Databricks Notebook Activity To Pipeline\n\nADF provides several Databricks-related activities, including Notebook, Jar, and Python activities, design to allow automated transformation of data using ADF and Databricks. In this exercise, you will add a Databricks Notebook activity to your ADF pipeline which will run the [Databricks-Data-Transformations.dbc]($./includes/Databricks-Data-Transformations) notebook, located in the `/includes` directory of the lab files you uploaded to your Databricks workspace.\n\nTake a few minutes to review the contents of this notebook to understand what transformations are being applied to the datasets being ingested into your Blob storage account via ADF. Also, notice that the notebook defines widgets named `accountKey`, `accountName`, and `containerName`. You will be passing these values in as parameters from ADF so the notebook can connect to your Azure Storage account.\n\n![ADF Widgets](https://databricksdemostore.blob.core.windows.net/images/03/03/adf-widgets.png \"ADF Widgets\")\n\nTo modify your pipeline, return to the ADF UI in a web browser and select the Author (pencil) icon in the left-hand menu, and then select your pipeline under Pipelines.\n\n![ADF UI Author blade](https://databricksdemostore.blob.core.windows.net/images/03/03/adf-author-blade.png \"ADF UI Author blade\")\n\nOnce there, expand Databricks under Activities, then drag the Notebook activity onto the design surface, and drop it to the right of the existing copy activity.\n\n![Add Databricks Notebook activity](https://databricksdemostore.blob.core.windows.net/images/03/03/adf-add-activity-databricks-notebook.png \"Add Databricks Notebook activity\")\n\nSelect the Notebook activity on the design surface to display tabs containing its properties and settings at the bottom of the screen, then enter the following:\n\n### General tab\n\n1. Enter \"LabNotebook\" into the Name field\n\n  ![Notebook Activity General Tab](https://databricksdemostore.blob.core.windows.net/images/03/03/notebook-activity-general.png)\n\n### Azure Databricks tab\n\n1. Select **+New** next to Databricks Linked Service to create a new Linked Service\n\n  ![Create New Linked Service](https://databricksdemostore.blob.core.windows.net/images/03/03/notebook-activity-azure-databricks-new.png \"Create New Linked Service\")\n\n2. On the New Linked Service dialog, enter the following:\n  - **Name**: enter a name, such as AzureDatabricks. \n  - **Databricks workspace**: Select the workspace you are using for this lab.\n  - **Select cluster**: Choose Existing interactive cluster.\n  - **Access token**: Paste the token you generated in step one into this field.\n  - **Existing cluster id**: Select the Databricks cluster you are using for this lab.\n  - Select **Test connection** and ensure you get a Connection successful message.\n  - Select **Finish**.\n  \n  ![New Linked Service (Azure Databricks)](https://databricksdemostore.blob.core.windows.net/images/03/03/adf-linked-service-databricks-notebook.png \"New Linked Service (Azure Databricks)\")\n\n3. You should now see the AzureDatabricks Linked Service selected in the **Databricks Linked Service** drop down.\n\n  ![Databricks Notebook activity Azure Databricks tab](https://databricksdemostore.blob.core.windows.net/images/03/03/adf-activity-databricks-notebook-databricks-tab.png \"Databricks Notebook activity Azure Databricks tab\")\n\n### Settings tab\n\n1. **Notebook path**: Enter \"/Users/[your-user-account]/03-Data-Ingestion-Via-ADF/includes/Databricks-Data-Transformations\" or select Browse, and select the `Databricks-Data-Transformations` notebook from the `/Users/[your-user-account]/03-Data-Ingestion-Via-ADF/includes` folder.\n2. Expand Base Parameters.\n3. Select **+New**.\n4. Enter \"accountName\" as the parameter Name, and the name of the Storage account you created into the Value.\n5. Select **+New** again, and enter \"accountKey\" as the Name of the second parameter, and paste in the Key for your storage account into the Value.\n6. Select **+New** again, and enter \"containerName\" as the name of the second parameter, and enter \"dwtemp\" for its Value.\n\n![Databricks Notebook activity Settings tab](https://databricksdemostore.blob.core.windows.net/images/03/03/adf-activity-databricks-notebook-settings.png \"Databricks Notebook activity Settings tab\")"],"metadata":{}},{"cell_type":"markdown","source":["## Step 3: Connect Copy Activity to Notebook Activity\n\nThe final step is to connect the Copy activity with the Notebook activity. Select the small green box on the right-hand side of the copy activity, and drag the arrow onto the Notebook activity on the design surface. Creating this connection sets the output from the copy activity as required input for the Databricks Notebook activity. What this means is that the copy activity has to successfully complete processing and generate its files in your storage account before the Notebook activity runs, ensuring the files required by that Notebook activity are in place at the time of execution.\n\nThe finished pipeline should look like the following.\n\n![Complete pipeline](https://databricksdemostore.blob.core.windows.net/images/03/03/data-ingestion-pipeline-complete.png \"Complete pipeline\")"],"metadata":{}},{"cell_type":"markdown","source":["## Step 4: Publish Pipeline\n\nIn the ADF UI toolbar, select **Validate All** to validate the pipeline, and then select **Publish All** to save the changes to the pipeline.\n\n![Publish All](https://databricksdemostore.blob.core.windows.net/images/03/03/adf-publish-all.png \"Publish All\")"],"metadata":{}},{"cell_type":"markdown","source":["## Step 5: Trigger and Monitor the Pipeline Run\n\nOnce the changes have been published, trigger the pipeline by selecting **Trigger**, then **Trigger Now** on your pipeline.\n\n![Trigger ADF Pipeline](https://databricksdemostore.blob.core.windows.net/images/03/03/adf-pipeline-trigger.png \"Trigger ADF Pipeline\")\n\nNext, navigate to the Monitor screen by selecting the Monitor icon in the left-hand menu.\n\n![Monitor ADF Pipeline](https://databricksdemostore.blob.core.windows.net/images/03/03/adf-monitor-pipeline.png \"Monitor ADF Pipeline\")\n\nSelect the View Activity Runs icon under Actions for the active pipeline run. Here you can observe the ADF pipeline activities running. Notice the Databricks Notebook activity will not run until of the copy activity has completed. Once done, you will see a Succeeded message next to each activity.\n\n![ADF Activity Runs](https://databricksdemostore.blob.core.windows.net/images/03/03/databricks-activity-runs.png \"ADF Activity Runs\")\n\nTo view the notebook output of the run, you can select the Output Action next to the LabNotebook activity, and then select the runPageUrl value. This will open the executed notebook within Databricks, and you can view details of the run in each cell's output.\n\n![Databricks Notebook Output](https://databricksdemostore.blob.core.windows.net/images/03/03/adf-output-databricks-notebook.png \"Databricks Notebook output\")"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n## Step 6: Verify transformations of data by looking at the generated table in Databricks\n\nOnce the ADF pipleline has finished running, you can check the table by running the following command.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/>Remember to attach your notebook to a cluster before running any cells in your notebook. In the notebook's toolbar, select the drop down arrow next to Detached, then select your cluster under Attach to.\n\n![Attached to cluster](https://databricksdemostore.blob.core.windows.net/images/03/03/databricks-cluster-attach.png)"],"metadata":{}},{"cell_type":"code","source":["# Display that list of tables, filtering for users, products, and weblogs tables, to ensure they aren't buried among other tables in the results\nsqlContext.tables().filter(\"tableName = 'homicides_2016'\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+---------+-----------+\ndatabase|tableName|isTemporary|\n+--------+---------+-----------+\n+--------+---------+-----------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["You should see the `homicides_2016` tables for `weblogs`, `users` and `products`. You can inspect the data in each table by running the cells below."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM homicides_2016 LIMIT 20"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Table or view not found: homicides_2016; line 1 pos 14\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:750)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:695)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:731)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:724)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:77)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:208)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:349)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:208)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:349)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:208)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:349)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:724)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:136)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:130)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:102)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:113)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:82)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:72)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:707)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:87)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:33)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:33)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:136)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:323)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:303)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:47)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:47)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:303)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:591)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:591)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:586)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:477)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:544)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:383)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:330)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:216)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'homicides_2016' not found in database 'default';\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.PoolingHiveClient.getTable(PoolingHiveClient.scala:42)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:176)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:176)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:141)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:104)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:139)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:345)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:331)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:137)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:175)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:763)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:763)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:141)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:104)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:139)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:345)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:331)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:137)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:762)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:143)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:736)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog$1.apply(Analyzer.scala:747)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog$1.apply(Analyzer.scala:747)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:746)\n\t... 99 more\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:122)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:136)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:323)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:303)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:47)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:47)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:303)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:591)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:591)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:586)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:477)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:544)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:383)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:330)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:216)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":10},{"cell_type":"markdown","source":["## Step 7: Aggregate the data\n\nNow that you have a normalized dataset containing the data for all cities, you can aggregate the data to compute the number of homicides per month, by city.\n\nStart by creating a new DataFrame containing the homicide data."],"metadata":{}},{"cell_type":"code","source":["homicidesDf = spark.sql(\"SELECT * FROM homicides_2016\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Perform a simple aggregation to see the number of homicides per month by city.\n\nAfter running the below query, play around with using Visualizations to represent the data, such as the following:\n\n  ![Databricks visualizations](https://databricksdemostore.blob.core.windows.net/images/03/03/databricks-aggregation-visualization.png \"Databricks visualizations\")"],"metadata":{}},{"cell_type":"code","source":["display(homicidesDf.select(\"city\", \"month\").orderBy(\"city\", \"month\").groupBy(\"city\", \"month\").count())"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Congratulations! \n\nYou have completed the Data Ingestion via ADF module.\n\nIn the lab you have learned how to use Azure Data Factory to ingest a remote dataset, and use Azure Databricks activities to address issues with the data, clean and format the data and load it into a global table to support downstream analytics."],"metadata":{}}],"metadata":{"language_info":{"codemirror_mode":{"name":"python","version":"3"},"mimetype":"text/x-python","name":"pyspark3","pygments_lexer":"python3"},"name":"03-Data-Transformation","notebookId":1395183622383453,"kernelspec":{"display_name":"PySpark3","language":"","name":"pyspark3kernel"},"celltoolbar":"Raw Cell Format"},"nbformat":4,"nbformat_minor":0}
